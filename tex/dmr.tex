\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,natbib,graphicx,amsthm,
  setspace,sectsty,anysize,dsfont,enumerate}

\usepackage{times}

\usepackage[svgnames]{xcolor}

\usepackage{lscape,arydshln,relsize,rotating}
\usepackage[small]{caption}
\usepackage{algorithm}

\newtheorem{prop}{\sc Proposition}[section]
\renewcommand{\qedsymbol}{}

\marginsize{1.1in}{.9in}{.3in}{1.4in}

\newcommand{\nb}{\color{blue}}
\newcommand{\dbl}{\setstretch{1.5}}
\newcommand{\sgl}{\setstretch{1.1}}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\bm}[1]{\mathbf{#1}}
\newcommand{\ds}[1]{\mathds{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\norm}[1]{|\!|#1|\!|_{1}}
\newcommand{\cd}[1]{{\tt#1}}
\newcommand{\e}[1]{{\footnotesize$\times10$}{$^{#1}$}}

\sectionfont{\noindent\normalfont\large\bf}
\subsectionfont{\noindent\normalfont\normalsize\bf}
\subsubsectionfont{\noindent\normalfont\it}

\usepackage[bottom,hang,flushmargin]{footmisc}

\pdfminorversion=4
\begin{document}

\sgl 

\pagestyle{empty}

~
\vskip 3cm

\noindent {\huge \bf Distributed Multinomial Regression} 

\vskip 1cm

\noindent{\Large Matt Taddy}

{\large
\vskip .5cm \noindent
{The  University of Chicago Booth School of Business}\\
{\tt faculty.chicagobooth.edu/matt.taddy}}



\vskip 2cm

{\noindent This article introduces a model-based approach to distributed
computing for multinomial logistic regression. We treat
counts for each response category as independent Poisson regressions
via plug-in estimates for fixed effects shared across categories.  The work is
driven by the high-dimensional-response multinomial models that
arise in analysis of a large number of random counts. Our archetypal
applications are in text analysis, where documents are tokenized
and the token counts are modeled as arising from a  multinomial dependent upon
document attributes. We estimate such models for a publicly available dataset
of reviews from Yelp, with text regressed onto a
large set of explanatory variables (user, business, and rating information).
The fitted models serve as a basis for exploring the connection between
words and variables of interest (e.g., star rating), for reducing dimension
into supervised factor scores, and for prediction. }
 

\newpage
\dbl

\pagestyle{plain}
\vskip 1cm
\section{Introduction}
\label{intro}


This article is motivated by datasets that include {\it counts} in a massive
number of {\it categories}, such as text corpora (counts for words), browser
logs (counts on websites), and website tracking (counts of clicks). The unit
upon which counts are observed -- e.g., a `document' for text or a `user' in
web analysis -- is annotated with {\it attributes}, additional information
about each document (author, date, etc) or user (age, purchases, etc).  Much
of contemporary Big data analysis involves some exploration, inference, and
prediction of, or controlling for, the relationship between these attributes
and the associated very-high-dimensional counts.


Say $\bm{c}_i$ is a vector of counts in $d$ categories, summing to $m_i =
\sum_j c_{ij}$, accompanied by a $p$-dimensional attribute vector $\bm{v}_i$ on observation
unit $i$ of $n$ total.  For example, in the archetypal text mining application, $\bm{c}_i$
are counts for words in document $i$ annotated with metadata $\bm{v}_i$.   We
connect attributes and counts through a big multinomial logistic regression
model,
\begin{equation}\label{bigmn}
\mr{p}(\bm{c}_{i} | \bm{v}_i,m_i) = 
\mr{MN}\left(\bm{c}_i;~\bs{\lambda}_{i}/\Lambda_i,
~m_i\right),~~\text{where}~~\lambda_{ij} = \exp[\alpha_j + \bm{v}_i'\bs{\varphi}_j]~~\text{and}~~\Lambda_i = \sum_{j=1}^d\lambda_{ij}.
 \end{equation} 
 The multinomial denoted $\mr{MN}$ here has, for unit $i$, category $j$ probability
$\lambda_{ij}/\Lambda_i$ and size $m_i$. This model can be computationally
expensive to estimate for a large number of response categories (i.e., big
$\bm{c}_i$ dimension $d$).  Even a single likelihood evaluation is costly, due
to the sum required for each normalizing {\it intensity} $\Lambda_i$. The
methodological innovation of the current article is to replace $\Lambda_i$
with initial estimates, then condition upon these plug-ins when estimating
(\ref{bigmn}) through $d$ individual Poisson regressions for counts in each
category $j$. This model-based factorization allows one to {\it partition}
computation across many independent machines, so with enough processors the
system of (\ref{bigmn}) is fit in the time required for a single Poisson
regression.

We refer to this framework as {\it distributed multinomial regression}, or DMR.
Our work here extends ideas from \cite{taddy_multinomial_2013}, which
introduced the strategy of {\it multinomial inverse regression} (MNIR).  That
article argues for estimation of models like (\ref{bigmn}) as the first step
in an inverse regression routine for predicting elements of new $\bm{v}_i$.
However, \cite{taddy_multinomial_2013} relies upon a fitting algorithm that
collapses response counts across equal $\bm{v}_i$, and hence scales {\it only}
for a small number of attributes (i.e., when $p$ is just one or two). That
article is also focused exclusively on applications in attribute prediction.
The purpose of the current article is thus two-fold: to supply techniques for
estimation when both $\bm{c}$ and $\bm{v}$ are high dimensional, and to
illustrate how these models can be useful in many aspects of analysis and
inference.

Much of the paper is devoted to an example analysis of reviews on Yelp -- an
Internet platform for feedback on various establishments, including
restaurants, barbers, schools and much else.  This dataset has a rich feature
set associated with a wide variety of reviews. The data are also publicly
available, after (free) registration on the data mining contest website
\cd{kaggle.com}. 
Moreover, our technology is provided in the \cd{distrom} package for R and
 Yelp analysis code is cataloged at \cd{github.com/mataddy/yelp}.  Public
access is essential here: our goal is to provide a
complete template for analysis of high-dimensional count data.

The estimation strategy is detailed in Section \ref{methods}, including model
factorization, plug-ins for $\Lambda_i$, and regularization
path estimation within each parallel regression. Methods
are illustrated in the short classification example of Section
\ref{FGL}, which shows utility for DMR not only in
big $d$ but also as a speedup for small $d$ multinomial regressions. Finally,
Section \ref{YELP} runs through our full Yelp example, detailing model
estimation and a variety of analysis applications.
\begin{itemize}\sgl
\item 
Exploration: what words are associated with funny or useful content?
\item Dimension reduction: which reviews have the most funny or useful content?
\item Prediction: what will be the usefulness or hilarity of a new review? 
\item Inference: does user experience lead to higher star ratings?
\end{itemize}
All of this is built upon {\it partial effects}:  connections
between text and attributes that arise  after controlling for
collinearity between attributes.  Section \ref{END} closes with a short
discussion.


\section{Methods: Estimation in Distribution}
\label{methods}

For convenience, we'll adopt terminology from text analysis for the remainder
and refer to each unit $i$ as a `document' and each category $j$ as a
`word'.\footnote{Even in text mining this is a simplification; each $j$ could
be a combination of words or any other language token.} Suppose that every
document-word count $c_{ij}$ has been drawn independently
$\mr{Po}\left(\lambda_{ij}\right)$ -- Poisson with intensity (i.e., mean)
$\lambda_{ij}$. The joint document likelihood, for $\bm{c}_i$, then factorizes as the
product of a multinomial distribution for  individual counts conditional on
 total count $m_i$ and a Poisson distribution $m_i$.
That is, the multinomial is `embedded' in the Poisson as
\begin{equation}\label{embed} \mr{p}(\bm{c}_{i}) = \prod_j
\mr{Po}\left(c_{ij};~\lambda_{ij}\right) =
\mr{MN}\left(\bm{c}_i;~\bs{\lambda}_{i}/\Lambda_i,
~m_i\right)\mr{Po}\left(m_i;~\Lambda_i\right). 
\end{equation}

This well known result has long been used by statisticians to justify ignoring
whether sampling was conditional on margin totals in analysis of contingency
tables. \cite{birch_maximum_1963} showed that the maximum likelihood estimate
(MLE) of $\bs{\lambda}_i$ is unchanged under a variety of sampling models for
3-way tables {\it under the constraint} that $\Lambda_{i} = m_i$. This is
satisfied at the MLE for a saturated model. \cite{palmgren_fisher_1981}
extends the theory to log-linear regression with $\lambda_{ij} = \exp[\alpha_j
+ \mu_i + \bs{\varphi}_j'\bm{v}_i]$, showing that the Fisher information on
  regression coefficients is the same regardless of whether or not you've
  conditioned on $m_i$  so long as $\mu_i$ in the Poisson model is estimated
  at its conditional MLE, 
 \begin{equation} \label{mlemu}
  \mu_i^\star =
\log\left(\frac{m_i}{\sum_j e^{\alpha_j + \bm{v}_i'\bs{\varphi}_j}}\right).  
\end{equation}

Most commonly, (\ref{embed}) is invoked when applying multinomial logistic
regression: totals $m_i$ are then ancillary and the $\mu_i$ drop out of the
likelihood.  Our DMR framework takes the opposite view: if we are willing to
fix estimates $\hat\mu_i$ potentially not at their MLE (we will argue for
$\hat\mu_i = \log m_i$), then the factorized Poisson likelihood can be
analyzed independently across response categories.\footnote{In an older version
of this idea, \cite{hodges_poisson_1960} introduce a Poisson approximation
to the binomial distribution, for which
\cite{mcdonald_poisson_1980} provides error bounds and extension to
multinomials.} As highlighted in the introduction, this yields distributed
computing algorithms for estimation on previously impossible scales.  Indeed,
we have observed in text and web analysis a recent migration from multinomial
models -- say, for latent factorization -- to Poisson model schemes; see
\citet{gopalan_scalable_2013} as an example.  From the perspective of this
article, such strategies are Big data approximations to their
multinomial precursors.


\subsection{Estimating baseline intensity}
\label{MU}

Parametrize the multinomial logistic regression model in (\ref{bigmn}) through
natural parameters $\eta_{ij} =
\alpha_{j} + \bm{v}_i'\bs{\varphi}_j = \log\lambda_{ij}$.  Then the negative
log likelihood is proportional to
\begin{equation}
\label{mnl} \sum_{i=1}^n\left[ m_i\log\left(\sum_{j=1}^d e^{\eta_{ij}}\right)
- \bm{c}_{i}'\bs{\eta}_{i} \right]. 
\end{equation} 
It is easy to verify that adding observation fixed effects $\mu_i$ to each
$\eta_{ij}$ in (\ref{mnl}) leaves the likelihood unchanged.  In contrast, the
corresponding Poisson model, unconditional on $m_i$, has negative log
likelihood proportional to 
\begin{equation} \label{pol}
\sum_{j=1}^d\sum_{i=1}^n\left[ e^{\mu_i + \eta_{ij}} - c_{ij}(\mu_i +
\eta_{ij}) \right] 
\end{equation} 
with gradient on each $\mu_i$ of $g(\mu_i) =
e^{\mu_i}\sum_j e^{\eta_{ij}} - m_i$, and  is clearly sensitive to these
observation `baseline intensities'.  As mentioned above, solution for the
parameters of $\eta_{ij}$ is unchanged between (\ref{mnl}) and (\ref{pol}) if
each $\mu_i$ is set to its conditional MLE in (\ref{mlemu}).

Unfortunately, if our goal is to {\it separate} inference for $\bs{\varphi}_j$
across different $j$, the MLE formula of (\ref{mlemu}) will create a
computational bottleneck: each category-$j$ Poisson regression requires
updates to $\bs{\mu}^\star = [\mu^\star_1 \ldots \mu^\star_n]'$ during
estimation.  Distributed computation precludes such communication, and
we instead use the simple plug-in estimator
\begin{equation}\label{plugin}
\hat \mu_i = \log m_i.
\end{equation}
 We'll justify this choice as optimal in a few simple models, and rely upon
  empirical evidence to claim it performs well in more complex
  settings.\footnote{ Note that, when compared to (\ref{mlemu}), the plug-in
  replaces $\sum_j e^{\alpha_j
+ \bm{v}_i'\bs{\varphi}_j} = 1$.  Adding a constant to each
  $\alpha_j$ leaves probabilities unchanged, so this can be made to hold without
  affecting fit.}  


The gradient of the Poisson likelihood in (\ref{pol}) on $\mu_i$ at our
plug-in is $g(\hat \mu_i) = m_i \left(\sum_i e^{\eta_{ij}}-1\right)$.  
Define the plug-in MLEs $\bs{\hat\eta}_{i}
  = [\hat\eta_{i1}\ldots\hat\eta_{id}]'$ as those which minimize the Poisson
  objective in (\ref{pol}) under $\mu_i=\hat\mu_i$.  Then in the
three simple settings below, $g(\hat
\mu_i)=0$ for $\bs{\eta}_i = \bs{\hat\eta}_{i}$. This implies that $\hat\mu_i$
is actually on the joint MLE, and thus that $\{\bs{\hat\eta}_{i},\hat\mu_i\}$ minimize the
 Poisson objective in (\ref{pol}) while $\{ \bs{\hat\eta}_{i}\}$ minimizes the logistic multinomial objective in (\ref{mnl}).
\begin{itemize}
\item In a saturated model, with
each $\eta_{ij}$ free, $\hat
\eta_{ij} = \log(c_{ij}) - \hat \mu_i = \log(c_{ij}/m_i)$ and $g(\hat
\mu_i) = 0$.
\item With intercept-only $\eta_{ij} =
\alpha_j$, the Poisson MLE is $\hat\alpha_j = \log \sum_i c_{ij} - \log
\sum_i e^{\hat\mu_i} = \log\left( \sum_i c_{ij}/M \right)$ where $M = \sum_i
m_i$, and $g(\hat \mu_i) = m_i(\sum_j \sum_i c_{ij}/M -1) = 0$.
\item Consider a single
$v_i \in
\{0,1\}$ such that $\eta_{ij} = \alpha_j + v_i \varphi_j$.  Write $C_{vj} = \sum_{i: v_i=v} c_{ij}$ and  $M_{v} = \sum_{i:
v_i=v} m_i = \sum_j C_{vj}$.  Then the Poisson MLE are $\hat\alpha_j =
\log(C_{0j}/M_0)$ and $\hat\varphi_j = \log(C_{1j}/M_1) - \log(C_{0j}/M_0)$,
so that  $g(\hat \mu_i) = m_i\left(\sum_j C_{v_ij}/M_{v_i} -1 \right) =0$.
\end{itemize}
Of course, these examples do not form a general result: the situation is more
complicated with correlated covariates or under regularization. But they
illustrate analytically why we might expect the performance we've seen
empirically: estimates based upon $\hat \mu_i = \log m_i$ do not suffer in
out-of-sample validation. The resulting benefit is huge, as using a plug-in
allows estimation of the Poisson regression equations to proceed in complete
isolation from each other.  See Appendix \ref{MR} for an example MapReduce implementation.

\subsection{Parallel Poisson regressions}
\label{GL}

Given baseline intensities fixed as $\hat \mu_i = \log m_i$, each of our $d$
separate Poisson regressions has negative log likelihood proportional to
\begin{equation}\label{obj}
l(\alpha_j, \bs{\varphi}_j) = \sum_{i=1}^n \left[ m_i 
e^{\alpha_j + \bm{v}_i'\bs{\varphi_j}} - c_{ij}(\alpha_j + \bm{v}_i'\bs{\varphi_j})\right].
\end{equation}
You are free to use your favorite estimation technique for each parallel
regression. This section outlines our specific approach:  `gamma lasso' $L_1$ regularized deviance minimization.

In high-dimensional regression, it can be useful to regularize estimation
through a penalty on coefficient size.   This helps  to avoid over-fit and
stabilize estimation.
A very common form of regularization imposes $L_1$ coefficient costs
\citep[i.e., the lasso of][]{tibshirani_regression_1996} which, due to
a non-differentiable cost spike at the origin, yields variable selection: some
coefficient estimates will be set to exactly zero.  Our results here use {\it
weighted $L_1$ regularization}
\begin{equation}\label{wl1}
 \hat\alpha_j,\bs{\hat\varphi}_j = \argmin_{\alpha_j,\bs{\varphi}_j} \left\{l(\alpha_j,\bs{\varphi}_j) + n \lambda \sum_{k=1}^p \omega_{jk} |\varphi_{jk} |\right\} ~~\text{where}~~\lambda,\omega_{jk} \geq 0.
\end{equation}
Penalty size $\lambda$ acts as a {\it squelch} that determines what you
measure as signal and what you discard as noise. In practice, since optimal
$\lambda$ is unknown, one solves a {\it regularization path} of candidate
models minimizing (\ref{wl1}) along the grid $\lambda_1 >
\lambda_2 \ldots > \lambda_T$.  Inference is completed through selection
along this path, with optimal $\lambda_t$ chosen to minimize cross validation
(CV) or information criteria (IC; e.g. Akaike's AIC) estimated
out-of-sample (OOS) deviance (i.e., to minimize the average error for a given
training algorithm when used to predict new data).  Crucially,  {\it selection is
applied independently for each category $j$ regression}, so that only a single
set of coefficients need be communicated back to a head node. 

Analysis in this article applies the {\it gamma lasso} algorithm  of
\citet{taddy_gamma_2013}, wherein weights $\omega_j$ diminish as a
function of $|\hat\varphi_j|$.\footnote{The iteratively reweighted least
squares algorithm in Section 6 of \citet{taddy_gamma_2013} applies directly to Poisson
family regressions by setting each iteration's `observation weights' $\lambda_{ij}$ and
`weighted response' $\log\lambda_{ij} + c_{ij}/\lambda_{ij} - 1$.}  In particular,
along the grid of $\lambda_t$ squelch values,
\begin{equation}\label{glweight}
\omega^{t}_{jk}  = \left(1 + \gamma
|\hat\varphi^{t-1}_{jk}|\right)^{-1} ~~\text{for}~~\gamma \geq 0.
\end{equation} 
This includes the standard lasso at $\gamma=0$.  For $\gamma>0$ it provides
{\it diminishing bias} regularization, such that strong signals are less
shrunk towards zero than  weak signals. This yields
sparser $\bs{\hat
\varphi}$, which reduces storage and communication needs, and can
 lead to lower false discovery rates.  

For selection along the path, we
minimize a {\it corrected AIC} \citep{hurvich_regression_1989}
\begin{equation}
\text{AICc:}~~~-2l(\hat\alpha_j,\bs{\hat\varphi}_j) + 2df_j\frac{n}{n-df_j-1},
\end{equation}
where $df_j$ is the estimated {\it degrees of freedom} used to fit
$\{\hat\alpha_j,\bs{\hat\varphi}_j\}$.  This corrects the AIC's tendency to
over-fit, and \cite{taddy_gamma_2013} finds that AICc performs well in a
variety of settings.   In Section
\ref{FGL}, where computation costs are very low, we also consider CV selection
rules: both CV1se, which chooses the largest $\lambda_t$ with mean OOS
deviance no more than one standard error away from  minimum, and CVmin,
which chooses $\lambda_t$ at lowest mean OOS deviance.

See \cite{taddy_gamma_2013} for details.\footnote{All of our results use the
{\tt gamlr} implementation in R.  The glass-shard example of Section \ref{FGL}
sets $\gamma=0$ for direct comparison to a lasso penalized alternative, while
the Yelp fits of Section \ref{YELP} all use $\gamma=1$ for more sparsity.}
That article reviews diminishing bias penalty regularization, emphasizing
connections to weighted $L_1$ penalties, the role of regularization paths and
model selection, and the distance from a weighted $L_1$ solution to an $L_0$
penalized oracle.


\section{Example: glass shards}
\label{FGL}


Our motivating big-$d$ applications have the characteristic that $m_i$ is
random, and usually pretty big.  For example, text mining $m_i$ is the total
word count in document $i$, and web analysis $m_i$ would be the total count of
sites visited by a browser.  A Poisson model for $m_i$ is not far fetched.
However, we also find that DMR also does well in the more common {\it
polychotomous regression} setting, where $m_i=1$ always. It thus provides an
every-day speedup in multinomial regression: even with small-$d$ response
categories, you'll be able to fit the model almost $d$ times faster in
distribution.\footnote{In shared-memory parallelization we observe
speedups close to linear in $d$, depending upon machine
architecture.} Thus before moving to our Yelp case study, we look at the
surprisingly strong performance of DMR in a simple classification problem.


This example considers the small {\it forensic glass} dataset from
\citet{venables_modern_2002}, available in the \cd{MASS} library for \cd{R}
under the name \cd{fgl}.\footnote{For the code used in this example, type
\cd{help(dmr)} in R after loading the \cd{distrom} library.}  The data are 214
observations on shards of glass. The response of interest is of 6 glass types:
window float glass (\cd{WinF}), window non-float glass (\cd{WinNF}), vehicle
window glass (\cd{Veh}), containers (\cd{Con}), tableware (\cd{Tabl}) and
vehicle headlamps (\cd{Head}).  Covariates for each shard are their refractive
index and \%-by-weight composition amongst 8 oxides. Figure \ref{fgl_coef}
shows Poisson regression regularization paths for each glass type,
with AICc selection marked by a vertical dashed line.


\begin{figure}[hb]
\includegraphics[width=6.25in]{../graphs/fgl_coef}
\caption{\label{fgl_coef} {\it Forensic Glass}. Regularization paths for each glass-type, with AICc selections marked.}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=5.75in]{../graphs/fgl_cv}
\end{center}
\vskip -.5cm
\caption{\label{fgl_cv}  {\it Forensic Glass}. OOS deviance samples for \cd{dmr} and \cd{glment} in a 20-fold OOS experiment.  }\end{figure}



The response here is a single category, such that $m_i = 1$ and
$\hat\mu_i=0$ for all $i$.  This clearly violates the assumption of Poisson
generation: $m_i=1$ is not random.  For example, Figure \ref{fgl_mu} shows the conditional MLE $\mu_i^\star = \log\left(m_i/\sum_j e^{\hat\alpha_j + \bm{v}_i'\bs{\hat\varphi}_j}\right)$ at AICc selected coefficients.  The result is distributed around, but not equal to, the assumed plug-in of $\hat\mu_i=0$ for all $i$.  However \cd{dmr} still works:  Figure \ref{fgl_cv} 
shows the distribution for OOS error in a 20-fold OOS experiment, either using AICc or CV selection {\it on each individual Poisson
regression}, against CV selected models from a lasso path for full
multinomial logistic regression as implemented in the \cd{glmnet} package for
\cd{R} \citep{friedman_regularization_2010}. There are subtle differences (e.g., AICc DMR selection has lower mean deviance with higher variance), but the full multinomial fits (\cd{glmnet}) do not have any clear advantage over the nearly $d$-times faster  approximation (\cd{distrom}).


\begin{figure}[h]\vskip .5cm
\hskip 4cm\includegraphics[width=3.25in]{../graphs/fgl_mu}
\caption{\label{fgl_mu} {\it Forensic Glass}.  The  conditional MLEs $\mu^\star_i$ implied at our DMR coefficient estimates.   }
\vskip -.25cm
\end{figure}


\section{Yelp case study}
\label{YELP}


These data were supplied by the review site Yelp for a data mining contest on
\cd{kaggle.com}.  The data are available at
\cd{www.kaggle.com/c/yelp\!-\!recruiting/data}, and code for processing
and estimation is  at
\cd{github.com/TaddyLab/yelp}.  We consider business, user, and review datasets
in the \cd{yelp\_training\_data} collection.  The reviews, for all sorts of
businesses, were recorded on January 19, 2013 for a sample of locations near to
Phoenix AZ.  The goal of the competition was to predict the combined number of
`funny', `useful', or `cool' (f/u/c) votes that a given review receives from
other users.  Such information can be used by yelp to promote f/u/c  reviews
before waiting for the users to grade them as such.


After detailing the  data and model in Section
\ref{YELP}.1, we describe a series of statistical analyses.
\begin{itemize}\sgl
\item[\ref{YELP}.2] Investigate model fit under a range of regularization
schemes, looking at how word loadings change with the relative  weight of
penalty on variables of interest vs controls.
\item[\ref{YELP}.3] Use the ideas of `sufficient reduction' to project text
through the model onto topics relevant to f/u/c votes or star ratings, and
interpret the resulting factor spaces.
\item[\ref{YELP}.4] Apply these factor projections in prediction for the
number of f/u/c votes (i.e., the original \cd{kaggle} task), and compare
results against a standard lasso in OOS experimentation.
\item[\ref{YELP}.5] Use the factor projections in treatment effect
estimation -- for the effect of user experience on rating -- where they serve
to control for heterogenaity in review content.
\end{itemize}
By viewing text data as a big multinomial regression, we are able to address
all of the above (and resolve the effects of many collinear attributes on
review text) through a single model fit.

\subsection{Data and model specification}

The data are $n=$ 215,879 reviews on 11,535 businesses by 43,873
users.\footnote{We've removed reviews with unknown user} Review text is split
on whitespace and tokenized into words (including combinations of punctuation:
potential emoticons). After stripping some common suffixes (e.g., `s', `ing',
`ly') and removing a very small set of stopwords (e.g., `the', `and', `or'),
we count frequencies for $d=$ 13,938 words occurring in more than 20 ($<
0.01\%$) of the reviews (total word count is $M=$ 17,581,214).  

\begin{samepage}
\noindent Metadata
includes review, business, and user attributes.
\begin{itemize}\sgl
\item \cd{stars}: review star rating (out of 5), from which we subtract the business average rating.
\item Review counts for \cd{funny}, \cd{useful}, or \cd{cool} votes.  We divide these by the square root of review age, which yields metrics roughly uncorrelated with the posting date.
\item \cd{usr.count}: a user's total number of reviews at time of posting the given review
\item \cd{usr.stars}: a user's average star rating across all of their reviews.
\item A user's average \cd{usr.funny}, \cd{usr.useful}, or \cd{usr.cool} votes
per review.
\item Business average star rating \cd{biz.stars} and review count \cd{biz.count}.
\item Business location amongst 61 possible cities surrounding (and including) Phoenix.
\item Business classification according to Yelp's non-exclusive (and partially user generated) taxonomy.  We track membership  for 
333 categories  containing more than 5 businesses. 
\end{itemize}\end{samepage}
This yields 405 variables for each review.  We also specify random effects for
each of the 11,535  businesses, leading to total attribute dimension
$p=$ 11,940. Data components are the $n \times d$ document-term matrix $\bm{C}$, the
$n$-vector of its row-totals  $\bm{m}$, and the $n\times p$ attribute matrix
$\bm{V}$.  

We split each row of the attribute matrix into two elements: $\bm{a}_i$, the
11 numeric review attributes from \cd{stars} through
 \cd{biz.count}, and $\bm{b}_i$, a length-11,929 vector of dummy indicators for  business identity,
 location, and yelp classification.    This is done to differentiate the
 variables we deem of primary interest ($\bm{a}_i$) from those which we
 include as controls ($\bm{b}_i$); write $\bm{V} = [~\bm{A}~ \bm{B}~]$ as the
 resulting partition. Columns of $\bm{A}$ are normalized to have mean zero and
 variance one.  The multinomial regression  of (\ref{bigmn}) is adapted by
 similarly splitting each $\bs{\varphi}_j = [ {\bs{\varphi}_j^a},
 {\bs{\varphi}_j^b}]$ and rewriting category intensities
$
\log \lambda_{ij} = \alpha_j + \bm{a}_i'\bs{\varphi}^a_j + \bm{b}_i'\bs{\varphi}^b_j.
$

\subsection{Multinomial model fit and interpretation}

Following the recipe of Section \ref{GL}, 
each word's Poisson regression is estimated 
\begin{equation}\label{yelpobj}
 \hat\alpha_j,\bs{\hat\varphi}_j = 
 \argmin_{\alpha_j,\bs{\varphi}_j} \left\{l(\alpha_j,\bs{\varphi}_j) + n \lambda \left[\sum_k \omega^a_{jk} |\varphi^a_{jk} | + \frac{1}{\tau}\sum_k \omega^b_{jk} |\varphi^b_{jk} |\right]\right\},
\end{equation}
where $l(\alpha_j, \bs{\varphi}_j) = \sum_{i=1}^n \left[ m_i e^{\alpha_j +
\bm{a}_i'\bs{\varphi}_j^a+ \bm{b}_i'\bs{\varphi}_j^b} - c_{ij}(\alpha_j +
\bm{a}_i'\bs{\varphi}_j^a+ \bm{b}_i'\bs{\varphi}_j^b)\right]$.
The {\it relative penalty weight} $\tau > 0$ controls differential
regularization between the target variables and the controls. At larger $\tau$
values, there is less penalty on $\bs{\varphi}_j^b$ and the effect of
$\bm{b}_i$ on $c_{ij}$ has less opportunity to pollute our estimate for
$\bs{\varphi}_j^a$. That is, $\bs{\hat\varphi}_j^a$ becomes more purely a {\it
partial effect}. At the extreme of $\tau = \infty$, any collinearity with
$\bm{b}_i$ is be completely removed from the estimated $\bs{\hat\varphi}_j^a$.

As outlined in Appendix \ref{MR}, counts for the $14k$ words are partitioned
into 128 files.  Each file is then read by one of 64 workstations, which
itself uses 16 cores in parallel to run through the Poisson regressions. Each
individual regression is a full gamma lasso path solution over grids of 100
$\lambda_t$ squelch values, with weights $\omega_{jk}^{at},\omega_{jk}^{bt}$
updated as in (\ref{glweight}) under $\gamma=1$, and AICc selected
coefficients are then written to file. The entire problem (including the
sufficient reduction projection of our next section) takes around 1/2 hour.


\begin{figure}[b!]
\hspace{-.25in}\includegraphics[width=6.75in]{../graphs/yelp_ir_paths}
\caption{\label{yelp_ir} {\it Yelp}.  
Poisson regression regularization paths for counts of the tokens \cd{:-)},
\cd{:-/}, and
\cd{:-(} under relative penalty weight $\tau=2$.  Coefficient values have been
multiplied by the corresponding covariate standard deviation.  The legend
highlights select covariates, regression degrees of freedom are on the top
axis, and our AICc selected estimates are marked with vertical dashed lines. }
\end{figure}


Regularization paths for a few of the Poisson regressions, estimated under
$\tau=2$ relative penalty weight, are shown in Figure \ref{yelp_ir}.
Coefficient values are scaled to the effect of 1sd change in
the corresponding attribute. We see, for example, that at our AICc selection
the effect of a 1sd increase in review stars multiplies the expected count (or
odds, in the multinomial model) for the happy face
  \cd{:\!-\!)} by around $\exp 0.38 \approx 1.46$, the `hmmm' face \cd{:\!-\!/} by $\exp
  -0.15 \approx 0.86$, and the sad face \cd{:\!-\!(} by $\exp -0.35 \approx 0.7$.
 Notice that \cd{:\!-\!/} and \cd{:\!-\!(} both occur more often in low star
 (negative) reviews, but that \cd{:\!-\!/} is associated with useful content while
 \cd{:\!-\!(} is uncool.


Table \ref{topwords} investigates fit under increasing $\tau$, which allocates
more  count variation to the controls in $\bm{B}$.  The numbers of nonzero
$\hat\varphi^a_{jk}$  (i.e., deemed useful for OOS prediction by AICc) are
decreasing with $\tau$ for all attributes.  This is because $\bm{B}$ accounts
for more variation in $\bm{C}$ at higher $\tau$, and  there is little residual
variation left for $\bm{A}$.  At $\tau=200$, for example, there are only 7
words positively associated with a
\cd{cool} vote. Such differential penalization is a powerful tool in Big data
analysis,  as it allows one to isolate partial effects in messy overdetermined
systems.  Here, $\tau=2$ yields top words only indirectly
associated with our attributes (e.g., {\it prik} is positive because Thai food
is tasty), while  full $\tau=\infty$ control leads to near perfect fit and
infinite likelihoods conditional on $\bm{B}$ alone. To our eye, $\tau=20$
manages a good balance: there remain many significant
$\hat\varphi^a_{jk}\neq 0$, but the model has avoided loading  words that
are not directly associated with the given attributes.  This fit is used in
the remainder of our study.

\begin{table}[b!]
\footnotesize\setstretch{1.25}
\hspace{-.4cm}
\begin{tabular}{cll|r}
& \small $\bs{\tau}$ & \small $\bs{\hat\varphi \neq 0}$ 
& \small {\bf top ten words by loading}\\
\noalign{\smallskip}
\hline
 & \multicolumn{2}{l|}{marginal}    & \footnotesize\it great love amaz favorite deliciou best awesome alway perfect excellent \\
\small +stars & 2 & 8440 & \footnotesize\it  unmatch salute :-)) prik laurie pheonix trove banoffee exquisite sublime \\
 & 20 & 3077 & \footnotesize\it  heaven perfection gem divine amaz die superb phenomenal fantastic deliciousnes \\
 & 200 & 508 & \footnotesize\it  gem heaven awesome wonderful amaz fantastic favorite love notch fabulou \\
\hline
 &  \multicolumn{2}{l|}{marginal}   & \footnotesize\it  not worst ask horrib minut rude said told would didn \\
\small -stars & 2 & 8440 & \footnotesize\it  rude livid disrespect disgrace inexcusab grossest incompet audacity unmelt acknowledge \\
 & 20 & 3077 & \footnotesize\it  rude incompet unaccept unprofession inedib worst apolog disrespect insult acknowledge \\
 & 200 & 508 & \footnotesize\it  worst horrib awful rude inedib terrib worse tasteles disgust waste \\
\hline
 & \multicolumn{2}{l|}{marginal}   & \footnotesize\it  you that know like your yelp ... what don who \\
\small funny & 2 & 6508 & \footnotesize\it  dimsum rue reggae acne meathead roid bong crotch peni fart \\
 & 20 & 1785 & \footnotesize\it  bitch shit god dude boob idiot fuck hell drunk laugh \\
 & 200 & 120 & \footnotesize\it  bitch dear god hell face shit hipst dude man kidd \\
\hline
 &  \multicolumn{2}{l|}{marginal}   & \footnotesize\it  that yelp you thi know biz-photo like all http :// \\
\small useful & 2 & 5230 & \footnotesize\it  fiancee rife dimsum maitre jpg poultry harissa bureau redirect breakdown \\
 & 20 & 884 & \footnotesize\it  biz-photo meow harissa www bookmark :-/ http :// (?), tip \\
 & 200 & 33 & \footnotesize\it  www http :// com factor already final immediate ask hope \\
\hline
 &  \multicolumn{2}{l|}{marginal}   & \footnotesize\it  yelp you that biz-photo http :// www know like your \\
\small cool & 2 & 4031 & \footnotesize\it  boulder lewi rogue lagunita wanton celebratory hanker politic mozzerella onsite \\
 & 20 & 577 & \footnotesize\it  userid htm cen rand poem sultry arlin brimm cubic inspiration \\
 & 200 & 11 & \footnotesize\it  biz-photo select yelp along certain fil chose house  \\
\hline\end{tabular}
\caption[l]{\label{topwords}
Top 10 words by loading on review characteristics, as a function of relative
penalty weight $\tau$.  The top row for each attribute  corresponds to terms ordered by
marginal correlations.}
\end{table}


\subsection{Sufficient reduction}

The coefficient estimates of the previous section, resolving the system of
direct relationships between words and attributes,  provide a rich basis for
story telling and exploratory analysis. For many, this is either the end goal
or a jumping-off point (e.g., to experimental testing for hypotheses generated
by this exploration).  But in our practice, the primary reason for fitting big
multinomial models is as a tool for {\it dimension reduction}, mapping from
the original $d$-dimensional text down to univariate indices that contain all
information relevant to a given attribute.  This section describes how the fitted model provides such a tool.



Another advantage of MNIR is business oriented: it is a practical way to
organize information.  In Big data environments, the $\bm{v}_i$ typically
contain subsets of attributes that will variously be treated as known or
unknown (i.e., conditioned upon or predicted) in future analyses.  For
example, at a marketing firm $\bm{c}_i$ could be counts for website visits by
a specific computer, while $\bm{v}_i$ contains geographic and demographic
information along with any observed purchasing behavior.
Given a fitted MNIR, the massive set of $\bm{c}_i$ counts can be
made available in compact $\bm{z}_i$ scores after mapping through
$\bs{\Phi}$.  For the analyst to include text data in any modeling related to
$\bm{v}_i$ they then need only grab the relevant dimensions of $\bm{z}_i$.
We'll see this tactic in practice in Section \ref{YELP}:  text
projections on review popularity used first in prediction, and then
augmented with projections onto user characteristics for an inference problem.
Since the estimated loadings $\bs{\phi}_k$ are partial effects --  influence of other attributes has been controlled for -- the SR projections will also correspond to partial rather than marginal association.

Consider the correlation matrices in Figure \ref{zcor}.  The original vote
attributes  are highly positively correlated, while the text projections are
either nearly independent (e.g., {\tt useful} against either \cd{funny} or
\cd{cool}) or strongly negatively correlated (\cd{funny} and \cd{cool}).  This
suggests that there are underlying factors that encourage {\it votes in any
category}; only after controlling for these confounding factors do we see the
true association between f/u/c content.  Similarly, all vote attributes are
uncorrelated with star rating, but for the text projections we see both negative
(\cd{funny},\cd{useful}) and positive (\cd{cool}) association.


\begin{figure}[htb]
\setstretch{1.1} \small 
\vskip .25cm
{\bf Correlation matrices}

\begin{center}
~~~~~~\begin{tabular}{r|ccccr|cccc}
 \multicolumn{4}{l}{\it attributes ($\bm{v}$)} & & 
 \multicolumn{4}{l}{\it \hskip 2cm text projections ($\bm{z}$)} \\ 
 %\multicolumn{10}{c}{}\\
\multicolumn{1}{c}{}& f & u &c & $\star$ &  \multicolumn{1}{c}{}&f & u &c & $\star$\\
\cline{2-5}\cline{7-10}
funny &  1 &  0.7 &  0.8 &   0 & \hskip 2cm funny &  1 &  -0.1 &  -0.7 &  -0.4\\
useful &  0.7 &  1 &  0.9 &   0 & \hskip 2cm useful &  -0.1 &  1 &  0.1 &  -0.2\\
cool &  0.8 &  0.9 &  1 &   0 & \hskip 2cm cool &  -0.7 &  0.1 &  1 &  0.5\\
stars &   0 &   0 &   0 &  1 & \hskip 2cm stars &  -0.4 &  -0.2 &  0.5 &  1\\
\end{tabular}
\end{center}
\caption{ \label{zcor} Correlation  for the original review attributes
in $\bm{a}$ (left) and for $\bm{z}$ (right) SR text projection. }
\end{figure}

The three 50-100 word reviews in Figure \ref{threerev} provide another
example.  A single review (bottom) on a historical site scores highest in
$a_{\tt funny}$ and $a_{\tt useful}$  attributes (and also
in $a_{\tt funny}$).  The review is neither dry nor useless, but we imagine
 it has a high vote count for a variety of unrelated reasons;
e.g., the page is heavily viewed, or people who read reviews of national parks
are more likely to vote. In contrast, read the two reviews identified through
our machine learned SR projections as having the most funny or useful text
content.  The funny review, for a pizza restaurant, is a fictional comedic
story. The useful review contains a high proportion of business photos ({\it
biz-photo}), which the multinomial model has identified as directly useful.



\begin{figure}[p]
\begin{center}\small
\framebox[0.95\textwidth]{
\parbox{0.9\textwidth}{
\setstretch{1.2}

\vskip .5cm
{\bf Funniest 50-100 word review, by SR projection $\bs{z_{\tt funny}}$.}

\vskip .2cm
{\it Dear La Piazza al Forno: We need to talk. I don't quite know how to say this so I'm just going to come out with it. I've been seeing someone else. How long? About a year now. Am I in love? Yes. Was it you? It was. The day you decided to remove hoagies from your lunch menu, about a year ago, I'm sorry, but it really was you...and not me.  Hey... wait... put down that pizza peel... try to stay calm... please? [Olive oil container whizzing past head] Please! Stop throwing shit at me... everyone breaks up on social media these days... or haven't you heard?  Wow, what a Bitch!}

\vskip .5cm
{\bf Most useful 50-100 word review, by SR projection $\bs{z_{\tt useful}}$.}

\vskip .2cm
{\it  We found Sprouts shortly after moving to town.  There's a nice selection of Groceries \& Vitamins.  It's like a cheaper, smaller version of Whole Foods.
[biz-photo] [biz-photo] We shop here at least once a week.  I like their selection of Peppers....I like my spicy food! [biz-photo][biz-photo][biz-photo] Their freshly made Pizza isn't too bad either. [biz-photo] Overall, it's a nice shopping experience for all of us. Return Factor - 100\%}

\vskip .5cm
{\bf Funniest and most useful 50-100 word review, as voted by Yelp users\\ (votes normalized by square root of review age).}

\vskip .2cm{\it
I use to come down to Coolidge quite a bit and one of the cool things I use to do was come over here and visit the ruins.  A great piece of Arizona history!  Do you remember the Five C's?  Well, this is cotton country. The Park Rangers will tell you they don't really know how old the ruins are, but most guess at around 600 years plus.  But thanks to a forward thinking US Government, the ruins are now protected by a 70 foot high shelter.  Trust me, it comes in handy in July and August, the two months I seem to visit here most.  LOL.  I would also recommend a visit to the bookstore.  It stocks a variety of First Nation history, as well as info on the area.
  http://www.nps.gov/cagr/index.htm.  While you are in Coolidge, I would recommend the Gallopin' Goose for drinks or bar food, and Tag's for dinner.  Both are great!}
\vskip .5cm
}}
\end{center}
\caption{\label{threerev} Illustration of the information contained in sufficient projections
$\bm{z}$.  The top two reviews are those, amongst all where $m\in (50,100)$,
with highest SR projection scores into the {\tt funny} and {\tt useful}
attribute spaces.  For comparison, we also show the single 50-100 word review
with highest values for both $v_{\tt funny}$ and $v_{\tt useful}$ (recall that
these are vote totals per square root review age).  Note that, since variance
of $\bm{z}$ increases with $m$, high scoring reviews tend to be longer.  One
can also, as in
\cite{taddy_multinomial_2013}, divide the SR projections by document length and work with normalized $z/m$.
On this scale, the funniest review is `{\it Holy Mother of God}' and the most
useful review is `{\it Ask for Nick!}'.}
\end{figure}


\subsection{Inverse regression for prediction}

Inverse regression (IR) is a general strategy for dimension reduction when
trying to predict $K$ dimensional attributes $\bm{v}_i$ from high dimensional
(say, of length $D \gg K$) covariates $\bm{x}_i$.  IR is a three stage
procedure: first, fit an {\it inverse model} for $\bm{x}_i | \bm{v}_i$;
second, use the inverse model to obtain {\it sufficient reduction} (SR) scores
$\bm{z}_i$ as reduced dimension projections from $\bm{x}_i$; third, fit a low
dimensional {\it forward regression model} for $\bm{v}_i | \bm{z}_i$.   In
this process the inverse distribution takes the form $\bm{x}_i \sim
f(\bs{\Phi}\bm{v}_i)$, where $f$ is some parametric (typically natural
exponential family) distribution that depends upon $\bm{v}_i$ through the
linear operator $\bs{\Phi}$, a $D\times K$ matrix of loadings.  The SR
projection is then the $K$-vector $\bm{z}_i = \bm{x}_i'\bs{\Phi}$. Under
certain conditions, it  holds that $\bm{x}_i \indep \bm{v}_i \mid \bm{z}_i$.
Thus all information in $\bm{x}_i$ relevant to $\bm{v}_i$ is summarized in
$\bm{z}_i$, and to predict $\bm{v}_i$ we need model only the $K$-variate
regression of $\bm{v}_i$ on $\bm{z}_i$. See \citet{cook_fisher_2007} for an
overview.

The reason one would want to use IR is its efficiency.  Say $D$, the dimension
of $\bm{x}_i$, is big relative to $n$ and that estimation of the parameters in
$\bm{v}_i |\bm{x}_i$ has a high sampling variance.  Assuming an inverse model
$f(\bm{x}_i | \bm{v}_i)$ introduces information into the estimation problem (a
less generous term is `bias') by restricting the relationship between
$\bm{x}_i$ and $\bm{v}_i$ to the possibilities parameterized by $f$.  If $f$
is close enough to the truth then this restriction will be useful.  Cook's
paper includes rigorous efficiency results.  We've found it helpful to make
analogy to the simple arguments used in the related setting of generative
classifiers \citep[e.g., as in][for Gaussian
discrimination]{efron_efficiency_1975}: you  assume higher asymptotic error
rates (since $\bm{x}_i$ is a higher dimensional random variable to be modeling
than $\bm{v}_i$) in exchange for getting near to the optimal rate faster
(e.g., through conditional independence assumptions in the inverse/generative
distribution).

\cite{taddy_multinomial_2013} introduced multinomial inverse regression 
as a version of IR for dealing with $\bm{x}_i = \bm{c}_i$, covariates that can
be characterized as high-dimensional count data.  Our primary application is
in text mining, where $\bm{c}_i$ is a vector of word (or token, phrase,
etc) counts and $D=\mr{length}(\bm{c}_i)$ is the size of the vocabulary.  The
model is just logistic regression, with $\eta_{ij} = \alpha_j +
\bm{v}_i'\bs{\varphi}_j$ the linear model parametrization for the $j^{th}$
response category (e.g., word) from the $i^{th}$ observation (e.g., document).
Given $\bs{\hat\Phi} = [\bs{\hat\varphi}_1 \cdots \bs{\hat\varphi}_D]'$
estimated for this logistic regression, MNIR sufficient reduction scores are
$\bm{z}_i = \bm{c}_i'\bs{\hat\Phi}/m_i$ and we have that $\bm{v}_i \indep
\bm{c}_i \mid \bm{z}_i, m_i$. The $1/m_i$ multiplier on here moves projection
from word counts to proportions, as in
\cite{taddy_multinomial_2013}, and sufficiency for $\bm{z}_i$ is dependent
upon $m_i$ because we've conditioned on these totals in the 
multinomial model.  The algorithm is completed with forward prediction for $\bm{v}_i$ given $[\bm{z}_i,m_i]$; Section \ref{YELP} uses both simple linear regression and a random forest algorithm.

Analysis, examples, applications of the MNIR framework are in
\cite{taddy_measuring_2013,taddy_multinomial_2013,taddy_rejoinder:_2013}. 
There are a number of benefits of the approach.  First, the IR efficiency
argument is especially simple: assuming a multinomial distribution
implies that each of the $M = \sum_i m_i$ count elements are independent
observations, such that the sample size for learning $\bs{\Phi}$ is now $M$
rather than $n$.  That is, in text mining, estimation variance decreases with
the number of words rather than the number of documents. Moreover,
as we see below via our use of random forests, working with the lower dimensional $\bm{z}_i$ scores allows us to apply powerful nonlinear and ensemble learners that would be infeasible (and unstable) on the full dimensional $\bm{x}_i$.

The final advantage of MNIR has been computational.
\cite{taddy_multinomial_2013,taddy_measuring_2013} work with very small
attribute sets, where $K<10$ and often $K=1$.  Since sums of multinomials with
equal probabilities are are also multinomial, one can then {\it collapse} the
modeled counts across equal level-sets of the covariates without affecting the
likelihood (continuous covariates can be binned to facilitate collapsing).
For example, if $v_i$ is a univariate binary attribute (e.g., whether a
speaker is Republican in the original MNIR paper) then the logistic multinomial regression
  has only two observations: the combined counts
corresponding to each of $\{i:v_i=0\}$ and $\{i:v_i=1\}$. Since collapsing can
occur independently for each dimensions of $\bm{c}_i$,  MNIR  even
works in settings where the original $n\times D$ count data is too big to
store on a single machine.

However, there are many settings with attributes of  larger dimension, say $K$
on the order of hundreds or thousands, where such collapsing is impossible.In
Section \ref{YELP} we have over 400 variables of meta-data associated with
each restaurant review.  Moreover,  the MNIR model is not very believable if
$K$ is small -- it seems implausible that only a few factors of interest
explain the heterogeneity between individual document word counts. The
arguments in \cite{taddy_multinomial_2013,taddy_rejoinder:_2013} explain how
MNIR can work even if the inverse model is misspecified on an individual level
(but is a good fit to the collapsed data).  However, there are many settings
where this will not hold, and the argument does not apply if you want to
interpret individual parameters. In such applications one needs to
include a big-$K$ set of explanatory attributes, possibly including many fixed
or latent effects.   This requires a new computational framework, and it has
driven development of DMR.  The distribution allows MNIR to work on a
previously impossible scale.

Turning to forward regression, denote combined f/u/c votes -- the response of
interest -- by the $n$-vector $\bm{y}$. We consider prediction of
$\log(y_i+1)$ using covariates $\bm{u}_i$, review length $m_i$, and the SR
projections $\bm{z}_i^v = (1/m_i)\bm{c}_i'\bs{\Phi}^v$, with $\bs{\Phi}^v =
[\bs{\varphi}^a_1 \cdots \bs{\varphi}^a_9]'$  coefficients for the nine
f/u/c$\times [1,t,t^2]$ variables in $\bm{v}_i$.  Note that we use text SR
factors only for variables derived from $y_i$; projection onto $\bm{u}_i$ is
ignored unless we want to reduce bias in estimation of
coefficients on  $\bm{u}_i$. Two algorithms are applied for predicting
$\log(1+\bm{y})$ from $[\bm{U},\bm{Z}^v,\bm{m}]$: ordinary least-squares (OLS), and
a 1024 tree random forest \citep[RF;][]{breiman_random_2001} as implemented in the
\cd{randomForest} package for \cd∫{R}. Results from a 20-fold CV (with each fold left-out for both IR and forward regression)
are shown in Figure \ref{yelp_cv}.  For comparison, we also
show results of straightforward lasso linear regression onto the 13,938 + 1 +
408 =  14,347 column input matrix $[\bm{C}/\bm{m},\bm{m},\bm{V}]$, where
rows of $\bm{C}/\bm{m}$ are word proportions $\bm{c}_i/m_i$.

\begin{figure}
\vskip -.2cm
\hskip .5in \includegraphics[width=5.5in]{../graphs/yelp_cv}
\vskip -.2cm
\caption{\label{yelp_cv} {\it Yelp}.  20-fold OOS  prediction of $\log(y_i +1)$, where $y_i$ is a reviews combined f/u/c votes.  For both inverse regression
(IR) procedures, penalization was selected inside the experiment via independent Poisson AIC.  The lines and points mark mean OOS MSE, and standard errors are too narrow to see. }
\end{figure}


Figure \ref{yelp_cv} shows that average performance of the IR-linear algorithm
(MNIR + OLS), using AIC model selection for IR coefficients, is no better than
the best straightforward lasso specification (although MNIR is being put to a
tougher test here: selecting penalization {\it within} the CV experiment
instead of finding the fixed penalty that performs best across folds). Such
performance is  typical of what we've observed in Big data application.
\footnote{In the comparisons of \cite{taddy_multinomial_2013}, the IR linear
model consistently outperforms a linear lasso comparator in OOS prediction and
in compute time.  However, the datasets used in that paper are both very
small, with $M\gg n$.  Thus the simple argument in the rejoinder
\cite{taddy_rejoinder:_2013} Proposition 1.1 -- that, for multinomial IR,
estimation variance decreases with $M$ instead of $n$ -- is working heavily in
favor of IR. Examples like the yelp application here are more typical in Big
data: even though $M > n$, the vocabulary size $D$ is smaller than $n$ and
straightforward linear regression is already plenty efficient.}   The
advantage of MNIR with linear forward regression is then computational: it
works in complete distribution  and is thus massively scalable.  However the
random forest results show another advantage of MNIR: one can deploy flexible
nonlinear machine learning on the low dimensional space resulting from SR
projection. Methods that are too computationally expensive or unstable on the
full covariate set are easy and fast to run on the reduced dimension subspace;
our experience with tree methods finds much rich interaction structure that
can be exploited after MNIR dimension reduction.

\subsection{Confounder adjustment in treatment effect estimation}

This reduces our confounder dimension from around 5 million down to 12624.

\begin{table}\small
\begin{tabular}{l|ccc}
& Marginal & Conditional on non-text covariates & Adding and interacting text SR \\
\cline{2-4}
Effect estimate & 0.003 & 0.015 & 0.019
\end{tabular}
\caption{Estimated effect of user experience (log number of reviews) on number of stars above business average.  Each corresponds to different levels of confounder adjustment, increasing from left to right.}
\end{table}



Machine learning feats are nice -- e.g., OOS $R^2$ greater than 50\% on the
log number of f/u/c votes -- but for staff at Yelp it is likely more important
to understand the sources of variation.  When a new review is written for a
specific business, Yelp can only promote (or not promote) it relative to
reviews on the same page.  The question is not `how popular will this review
be?', but rather `should we promote this review over others for this
business?'  One way to answer looks at the users: are there certain people who
write better reviews?  This creates an inference problem -- we want to
estimate the real effect of user characteristics. This section first describes
effect estimation under the setup of Section \ref{YELP}, before investigating how
inference can be improved by conditioning on additional SR projections.

Regularization paths for
Poisson regression (full generalized linear model, not a linear model for
log $y$) of  counts $y_i$ on   $\bm{u}_i$ and
$\bm{z}^a_i$.  This is a gamma lasso fit with $\gamma =10$, for a very concave
penalty (little bias), such that estimates at the end of the path
are near the corresponding MLE.  As in any multivariate regression, these coefficients
represent {\it partial effects} --  change due to
 individual covariates {\it after} controlling for the effect of all other
inputs.  Consider the coefficient on \cd{usr.cool}, a user's historical
count of `cool' votes.  This has a negative partial effect (around -0.17 at
minimum penalty), despite having a big positive marginal effect (if you follow
the path, you'll see that \cd{usr.cool} is actually the first covariate to
enter the model).  Thus after you account for the quality
predictable by funny or useful votes (particularly useful, which is about
twice as common as the others), extra cool votes indicate someone less likely
to write popular reviews.  This situation occurs because the individual
category votes are highly collinear, with correlations between each other of
0.7 to 0.9.

Inference about regression parameters is notoriously difficult in the presence of high dimensional collinear controls.  Likelihood maximization for the full 14,347
variable regression in our Yelp example, of vote count on all attributes and all text counts,  would be very expensive and implies only around 15 observations per covariate.  
In larger problems, $n$ and $\mr{dim}(\bm{x}_i)$ may be so large that such a regression is computationally impossible.
Unfortunately, the tools that we would usually employ for variable selection (such as a straightforward lasso) will yield biased effect estimates.  To see this, consider
the regression 
\begin{equation}\label{teffects}
\ds{E}[\log y_i] = \beta_0 + d_i\theta  + \bm{x}_i'\bs{\beta}
\end{equation}
where $d_i$ is our `treatment' (e.g., user characteristic), $\theta$ is
the effect of interest, and $\bm{x}_i$ is a large vector of control
variables (e.g., all other review attributes and the token counts).   There
will surely be elements of $\bm{x}_i$ that are correlated with $d_i$. If
we estimate $\bs{\beta}$ under lasso penalization, 
then $\hat\beta_j \neq 0$ only if there is 
residual effect of $a_{ij}$ beyond that which can be regressed on $d_i$.  The
 effect for these treatment-collinear variables will be conflated
into $\hat\theta$, leading it to be mis-estimated simply because $d_i$ is correlated with many
variables who's effect has been underestimated.  The efficiency of the lasso
for prediction is now working against us in inference.

In contrast, because the text covariates, $\bm{c}_i$, enter only through the 9
dimensional SR projection $\bm{z}^a_i$, we are able to use likelihood maximization
 to estimate the partial effects.
We can improve these estimates still by adding the SR
projections $z^u_{ik}$ corresponding to the treatments of interest.    To see why, look
again to the treatment effects regression in (\ref{teffects}): 
unbiased estimation of $\theta$ needs $\bs{\hat\beta}$ to account both for
predictability of $\log y_i$ from $\bm{x}_i$ and of $d_i$ from $\bm{x}_i$.  To
put it another way, an ideal version of (\ref{teffects}) would have $\ds{E}[\log y_i] = \beta_0 + d_i\theta  + \beta_y \ds{E}[\log y_i
| \bm{x}_i] + \beta_d \ds{E}[d_i | \bm{x}_i]$.  Without the $\beta_d \ds{E}[d_i | \bm{x}_i]$ 
term, estimated $\hat\theta$ will be confounded with any effects from
$a_{ij}$ that can be explained by the correlated bit of $d_i$. 

For the same reasons, including in forward regression both $\bm{z}_i^v$ {\it
and} the relevant elements of $\bm{z}_i^u$, say $\bm{z}_i^d$ where $\bm{d}_i$
contains the \cd{user.*} elements of $\bm{u}_i$, more fully controls for the
confounding information available in review text. This is straightforward to
demonstrate through the MNIR sufficiency results.  The model  has $y_i \indep
\bm{c}_i | \bm{z}^a_i, \bm{u}_i, m_i$, while an augmented {\it treatment
effects MNIR} forward model has $y_i, \bm{d}_i \indep
\bm{c}_i | \bm{z}^a_i,\bm{z}^d_i, \bm{u}_i\!\setminus\!\bm{d}_i, m_i$ with
$\bm{u}_i\!\setminus\!\bm{d}_i$ the  elements of $\bm{u}_i$ that are
considered controls. In the latter procedure, it is  the {\it joint}
distribution of $y_i$ and $\bm{d}_i$ that is made conditionally independent of
the text information.

\section{Discussion}
\label{END}

Distributed estimation for big multinomial regression opens the opportunity
for such models to be applied on a new  scale, one that is limited only by the
number of machines you are able procure. This advance is important for applied
work.  The collapsing of
\cite{taddy_multinomial_2013} is useful in prediction problems, but today
we more often find ourselves addressing inference questions.  High dimensional
attribute sets are essential for such problems because multinomial model needs
to believable. This is in contrast to pure prediction problems where, as
outlined in \cite{taddy_rejoinder:_2013}, one can model inverse regression
misspecification during forward regression.

Finally, the message of this paper has been that Poisson factorization
enables fast estimation of multinomial distributions.  It has been pointed out
to us that, in unstructured data analysis, a Poisson seems little more
arbitrary than a multinomial model.  Equation (\ref{embed}) clarifies this
issue:  the only additional assumption one makes by working with independent
Poissons is that the aggregate total, $m_i$, is Poisson.  We've attempted to
mitigate the influence of this assumption, but that is unnecessary if you
consider the Poisson a fine model in and of itself.

\appendix
\section{Appendix}

\subsection{MapReduce} 
\label{MR}

  MapReduce \citep[MR;][]{dean_mapreduce:_2004} is a  recipe
for analysis of massive datasets, designed to work when the data itself is
distributed: stored in many files on a network of distinct machines.
The most common platform for  MR is Hadoop paired with a distributed file-
system (DFS)  optimized for such operations (e.g., Hadoop DFS or
Amazon's S3 storage).

A MapReduce routine has three main steps: map, partition, and reduce.  The
partition  is handled by Hadoop, such that we need worry only
about map and reduce.  The map operation parses  unstructured data into a
special format.  For us, in a text mining example, the mapper program will
take a document as input, parse the text into tokens (e.g. words), and output
lines of processed token counts: `\cd{token   document|count}'.  The pre-tab
item (our \cd{token}) is called a `key'.  Hadoop's sort facility uses these
keys to send the output of your mappers to machines for the next step,
reducers, ensuring that all instances of the same key (e.g., the same word)
are grouped together at a single reducer.  The reducer then executes some
operation that is independent-by-key, and the output is written to file
(usually one file per reducer).

DMR fits nicely in the MR framework.  Our map step tokenizes your unstructured data  and organizes the output by token keys.  Reduce then takes all observations on a single token and runs a Poisson log regression, applying the gamma lasso with IC selection to obtain coefficient estimates.  These are used to build our SR scores, which can be employed in forward regression after the MR routine is done.  This recipe is detailed in Algorithm \ref{mralgo}.

\begin{algorithm}[htb]
\caption{\label{mralgo} MapReduce DMR }
\vskip .25cm
{\bf Map:}  For each document, tokenize and count sums for each token.  Save the total counts $m_i$ along with attribute information $\bm{v}_i$. Output \cd{token   document|count}.

\vskip .25cm
Combine totals $m_i$ and attributes $\bm{v}_i$ into a single table, say \cd{VM}. This info can be generated during map or extracted in earlier steps.  Cache \cd{VM} so it is available to your reducers.

\vskip .25cm
{\bf Reduce:} For each token key `$j$', obtain a regularization path for Poisson regression of counts $c_{ij}$ on attributes $\bm{v}_i$ with $\hat\mu_i = \log m_i$.  Apply AICc to select a segment of coefficients from this path, say $\bs{\hat\varphi}_j$, and output nonzero elements in sparse triplet format: \cd{word|attribute|phi}. 

\vskip .25cm
Each reducer writes coefficients $\bs{\hat\varphi}_j$ of interest to file, and maintains a running total for SR projection, $\bm{z}_i +\!\!= \bm{c}_i'\bs{\hat \varphi_j}$, output as say \cd{Z.r} for the $r^{th}$ reducer.  When all machines are done we aggregate \cd{Z.r} 
to get the complete projections.  

~
\end{algorithm}



We've written this as a single MR algorithm, but other variations may work better for your computing architecture. Our most common implementation uses streaming Hadoop on Amazon Web Services (AWS) to execute the map on a large number of files in AWS S3 cloud storage, but replaces the regression reduce step with a simple write, to solid state storage `midway' at the University of Chicago's Research Computing Center, of token counts tabulated by observation.  For example, given 64 reducer machines on AWS the result is 64 text tables on midway, with lines `\cd{word|doc|count}', each containing {\it all} nonzero counts for a subset of the vocabulary of tokens.  These files are small enough to fit in working memory\footnote{If not, use more reducers or split the files.} and can be analyzed on distinct compute nodes, each employing another layer of parallelization in looping through Poisson regression for each token.  This scheme is able to take advantage of Hadoop for fast tokenization of distributed data, and of
high performance computing architecture (much faster than, say, a virtual AWS instance) for distributed  regression analyses.  This is a model that should work well for the many statisticians who have access to computing grids that are designed for high throughput tasks more traditionally associated with physics or chemistry.

\sgl\small
\bibliographystyle{asa}
\bibliography{dmr}


\end{document}
